
# ⚡ Hackathon Project Template ⚡
_This is a sample submission repository.
Please [__fork this repo__](https://help.github.com/articles/fork-a-repo/) and use this as a starting point for your hackathon project._

## Project Name


### Team name

##### Region location

##### Team Members
- Michael Accetto, Developer, Designer, coffee maker

#### Project Description

In today's digital age, our interactions with digital assets are increasing at an exponential rate. To ensure these digital assets serve their purpose, i.e., desired user interaction and its following outcomes, it's imperative that they are perceived by users and in turn, stimulate them, as intended. This puts the design of these assets as the topmost priority.

EYEFX measures where the users are looking on the computer screen, for how long, and where do they go next. This provides valuable insights into the perception and interaction of users of these digital assets which can be used to optimize design towards specific targets. Through the Effect Force platform, a requester can submit their design and have it trialled by a number of users, throughout the world, otherwise impossible to reach in a practical fashion. Moreover, pooling thousands of users' interactions enables Machine Learning techniques to validate and combine the data in order to improve detection algorithms over time.

In practice, the campaign works by recording eye tracking data while a specific task (e.g. finding information) is performed on the desired webpage. Once the task is completed,  the worker will be prompted to complete the task by answering one or more questions. This makes it possible to relate the eye tracking data to the quality of the answer given by the worker, further increasing the insights obtainable from the campaign results.

Under the hood, EYEFX uses the Webgazer library, developed by the [Brown HCI](https://hci.cs.brown.edu/) lab. Its functioning relies on Machine Learning to generate predictions of the observed section of the screen surface from eye features. Additionally, head positioning is monitored and validated, while optionally showing feedback on screen.
Visual design is handled by bootstrap, while object interactions takes advantage of jQuery.
Hidden input fields are used to submit the task in a string of text, JSON formatted, containing x and y coordinates for each timestep. The [Effect SDK](https://developer.effect.network/) is used to interact with [TEN](https://effect.network/download/effect_whitepaper.pdf).

#### Summary
The effect workforce would use our AI-based eyetracking program to collect and synthesize data from campaigns and present customers (websites, adTech, app devs) with actionable insights that will guide their design efforts towards fruition. This provides an opportunity to tap into the 41.8 bn USD (just U.S) web design services market and contribute to the web dev and digital design job market already at a 13% growth rate.

#### URLs

N/A

#### Presentation

[3 Minutes Video](https://youtu.be/TXW0R2PfgYY) 

[4 Minutes Video](https://youtu.be/uXlsayVKdrA)

#### Next Steps

In the future, eye tracking accuracy could increase thanks to the combination of thousands of eyetracking data available throughout the world. More echnical, simpler improvements are also possible, such as Full Screen Mode to offer a greater tracking surface, conversely dicreasing distactors and inter-browser differences. Lastly, the recent increase of biometric (Infra Red) cameras in consumer-grade devices opens avenues of potential improvement for webcam based eye tracking technologies. 

#### License
This repository includes an [unlicensed](http://unlicense.org/) statement though you may want to [choose a different license](https://choosealicense.com/).
